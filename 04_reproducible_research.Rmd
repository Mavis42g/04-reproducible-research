---
title: "Week 4 Tutorial: Reproducible and reusable research"
author: "Eliza Harris"
date: "2022-10-11"
output: html_document
---

```{r echo=FALSE, message=FALSE}
require(lubridate)
#require(tidyverse)
require(purrr)
```

In this tutorial, we will set up a basic MCMC to look at the process and examine the results. In the exercise, we will further investigate the MCMC and look at the influence of different set up choices. In the tutorial and exercise, we will only use synthetic data examples, to better illustrate how the MCMC functions.  

We will use some of the functions and methods we saw in the lectures to construct our code, and the code will be commented and structured to illustrate the best practice principles that underpin code reproducibility. 

## Creating a synthetic dataset

We are using an MCMC to relate an independent variable (x), to a dependent variable (y), according to a linear relationship (y = x*p + 2). For example, x could represent temperature and y could represent plant biomass growth. We therefore need to create:

* A set of observations of quantity x, which have a Gaussian error distribution with a standard deviation of 2.

```{r}
x = c(5,6,7,8,9,7,5,4,3,2,1,5,6,5,6,5,7,8,9)
x_err = 2
```

* An uncertain parameter (p), which has a Gaussian error distribution. Our prior estimate of p is 10 with an uncertainty of 2.

```{r}
p = 10
p_err = 2
```

* A set of observations of quantity y (y_obs), which are created here based on x, and have a Gaussian error distribution with a standard deviation of 2. We also add noise to y using rnorm(). Note that we created y_obs with p=3.6, whereas the prior estimate of p is 10. The MCMC will reconcile these two estimates to produce the posterior value of p which is most consistent with all input data.

```{r}
y_obs = x*3.6 + 2 + rnorm(length(x),0,2)
y_obs_err = 2
```

* A model relating stating that quantity y follows the relation y_mod = x*p + 2. We are not so confident in the model and assign its predictions an uncertainty of 2.

```{r}
test_model = function(x,p){ 
  return(x*p+2)
}
model_err = 2
```

## Investigate the synthetic dataset

```{r}
# Create a function to compare a model and observations
compare_obs_model = function(obs,obs_err,model){
  par(mar=c(2,4,1,1)) # set plotting margins
  par(mfrow=c(2,1))
  # First, let's plot both observations and model of y
  plot(x,model,ylab="y",pch=20,col="blue")
  points(x,obs,ylab="y",pch=20,col="magenta")
  legend(1, 95, legend=c("Model", "Obs"),
       col=c("blue", "magenta"), lty=1, cex=0.4)
  # Now we'll plot a 1:1 plot
  plot(model,obs,ylab="observations",xlab="model",pch=20,col="blue")
  lines(obs,obs,col="red")
  # R has no built in error bars, but we can approximate them with arrows to show the uncertainty in our observations
  arrows(x0=model, y0=obs-obs_err, x1=model, y1=obs+obs_err, code=3, col="blue", lwd=0.5, angle=90, length=0.1, )
  # We'll also print the root mean squared error
  rmse = (sum((obs-model)**2)/length(obs))**0.5
  print(paste0("RMSE: ",rmse))
}

# Run and check the model
y_mod = test_model(x,p)
compare_obs_model(y_obs,y_obs_err,y_mod)
```

We can see that the model is doing a poor job of reproducing the observations. This is expected, since we have the parameter p set to 10, but we know that in this synthetic dataset, the relationship is actually 3.6. We'll use an MCMC to see if we can improve our estimates of x, y and p.

## Set up the parameters and functions for the MCMC

For the MCMC, we need to set a few inputs:

* The step length describes how "far" the random walk steps in each iteration. In this example, we will "move" our inputs by (one standard deviation) x (step length) x (random number) in each iteration. In the exercise, we'll examine the impact of varying the step length.

* The number of iterations states how often we will "roll the dice" and run the MCMC.

* We will also create space for the perturbed values of each input at each step.

The principle of the MCMC is to:

* Take a "step" (in the random walk) and perturb each input to a new value

* Rerun the model describing the relationship under investigate

* Check if the new values for each input and the new model outputs have a relative probability (compared to the previous step) larger than a uniformly distributed random number between 0 and 1 - the "Metropolis Hastings rule", and *accept* them if they do. The M-H rule means that we do not only accept new steps that improve our results, but also sometimes steps that worsen our results, thus avoiding becoming stuck in local minima.

* If the step was accepted, it becomes the starting point for our next step. If not, we return to the most recent accepted value to take a new step. Therefore, we keep track of the most recent accepted values in the MCMC.

```{r}
# Set parameters of the MCMC
step_length = 1 # This designates the length of the "step" in the random walk MCMC
n_iterations = 10000 # How many "steps" we will take

# Starting point for the MCMC: The first step begins at our initial estimates. We will overwrite these variables each time we accept a solution.
prev_x = x
prev_y = y_obs
prev_p = p
prev_y_mod = test_model(x,p)

# Create space for the results of the MCMC
x_i = matrix(nrow=n_iterations,ncol=length(x),-1) # Values of x at each iteration
y_est_i = matrix(nrow=n_iterations,ncol=length(y_obs),-1) # Values of y at each iteration
y_mod_i = matrix(nrow=n_iterations,ncol=length(y_obs),-1) # Modelled values of y at each iteration
p_i = matrix(nrow=n_iterations,ncol=1,-1) # Values of p at each iteration
accepted = matrix(nrow=n_iterations,ncol=1,-1) # Signify if iteration accepted or not; 1 = accepted, 0 = not accepted, -1 = initial value (shows us if iteration was run)

# Function to find the probability that x belongs to a specified Gaussian distribution
gaussian_prob = function(x,m=0,s=1){
  res = exp( -0.5 * ((x-m)/s)**2 )
  if (length(res)>1){ # For vectors, return just one result 
    res = mean(res)
  }
  return(res)
}
```

## Run the MCMC

Now we will move through the MCMC. It is recommended to follow the loop step-by-step in the console to better understand the code. Set n=1 and execute the code line by line in the console, looking at the output of each line to fully understand what is being calculated.

```{r}
# Run the MCMC
for (n in 1:n_iterations){
  accept = 0
  
  # The first step of each iteration is to perturb the estimates of x, y and p according to the step size and their uncertainty
  x_i[n,] = prev_x + x_err*runif(1,-1,1)*step_length
  y_est_i[n,] = prev_y + y_obs_err*runif(1,-1,1)*step_length
  p_i[n] = prev_p + p_err*runif(1,-1,1)*step_length
  
  # Next, we decide if we "accept" the new values of x, y and p. This is done like rolling a dice, according to the Metropolis-Hastings rule. We generate a random number between 0 and 1. If the ratio of the probability of y_i to the probability of y_prev is larger than the random number, we accept the values. Similarly for x and p.
  current_prob_x = gaussian_prob(x_i[n,],x,x_err)
  prev_prob_x = gaussian_prob(prev_x,x,x_err)
  if (runif(1,0,1)<current_prob_x/prev_prob_x){accept= accept + 1}
  current_prob_y = gaussian_prob(y_est_i[n,],y_obs,y_obs_err)
  prev_prob_y = gaussian_prob(prev_y,y_obs,y_obs_err)
  if (runif(1,0,1)<current_prob_y/prev_prob_y){accept= accept + 1}
  current_prob_p = gaussian_prob(p_i[n],p,p_err)
  prev_prob_p = gaussian_prob(prev_p,p,p_err)
  if (runif(1,0,1)<current_prob_p/prev_prob_p){accept= accept + 1}
  # If all three new input datasets/values were accepted, accept is == 3
  
  # If all the new values were accepted, we run the model. We don't do this in every iteration, because running the model is usually the most computationally expensive part of the MCMC (not in this case, it is a tiny model).
  if (accept==3){
    y_mod_i[n,] = test_model(x_i[n,],p_i[n])
    # Test agreement between the modelled results and the estimation of y in this iteration
    current_prob_m = gaussian_prob(y_mod_i[n,],y_est_i[n,],model_err)
    prev_prob_m = gaussian_prob(prev_y_mod,y_est_i[n,],model_err)
    if (runif(1,0,1)<current_prob_m/prev_prob_m){accept= accept + 1}
  }
  
  # Now we have completed the calculations of this iteration of the MCMC
  # If x, p, y and the model results were accepted, we use these values as the starting point of the next iteration
  if (accept==4){
    accepted[n] = 1
    prev_x = x_i[n,]
    prev_y = y_est_i[n,]
    prev_p = p_i[n]
    prev_y_mod = y_mod_i[n,]
  } else {
    accepted[n] = 0
  }
}

# Check the MCMC ran correctly
print(paste0(n_iterations," iterations were run"))
print(paste0(sum(accepted==-1)," iterations did not run correctly"))
print(paste0(sum(accepted==1)," iterations were accepted"))
print(paste0(sum(accepted==1)/n_iterations*100,"% of iterations were accepted"))
```

## Examine the MCMC results

Now we have run the MCMC. Each iteration of the MCMC improved our results in line with Bayes' theorem: We considered our previous best estimates and their uncertainties, and updated our estimates to achieve better agreement between the model and the observations. We didn't treat either the model or the observations as true - both of these are uncertain. Let's see what our `posterior' results look like for the parameter "p".

```{r}
par(mfrow=c(1,1))
plot(1:n_iterations,p_i,pch=4,col="blue")
points((1:n_iterations)[accepted==1],p_i[accepted==1],pch=4,col="red")
lines(1:n_iterations,rep(p,n_iterations),col="yellow") # Plot prior p also
legend("topleft", legend=c("prior","all solutions","accepted solutions"),col=c("yellow", "blue","red"), lty=1, cex=0.5, pch=20)
p_post = c(mean(p_i[accepted==1]),sd(p_i[accepted==1]))
print(paste0("Posterior value of p: p = ",round(p_post[1],2)," +/- ",round(p_post[2],2)))
```

The posterior (optimised) value of p is approximately 9 +/- 2 (results will be slightly different every time as we did not set the random seed). The optimised value is between what we generated the data with (3.6) and our prior estimate (10). The MCMC does not return the value of 3.6 that we used to generate y_obs, because this would ignore the prior estimate. The MCMC is able to integrate uncertainties from many sources.

We could also have found p using a linear regression between x and y.

```{r}
# We will set the intercept to 2, the same as in the MCMC
lm_fit = lm((y_obs-2) ~ 0 + x)
summary(lm_fit)
```

The linear fit finds a slope (p) close to the value of 3.6 that we used to generate y_obs from x. The linear fit is unable to account for our confidence in each of the inputs, x, y, p and the model. The linear fit function lm can accept weights for different points, but in this case all our points have the same weight - this also only represents uncertainty in one dimension. We could use a linear fit with bootstrapping or another method to account for uncertainty, however the MCMC is a flexible way to accomplish this task. 

A common assessment of MCMC results is to plot the independent variable (y) against the dependent variable (x) for the prior and the posterior scenarios, and additionally to include a 1:1 plot of modelled and observed/estimated y for the prior and posterior scenarios.

```{r}
x_post = apply(x_i[accepted==1,],2,mean) # use "apply" to apply the function "mean" to each column (2 = by column, 1 = by row) 
x_post_sd = apply(x_i[accepted==1,],2,sd)
y_post = apply(y_est_i[accepted==1,],2,mean) 
y_post_sd = apply(y_est_i[accepted==1,],2,sd)
y_mod_post = test_model(x_post,p_post[1])

compare_post_prior = function(x,y_obs,y_mod,x_post,y_post,y_mod_post){
  par(mar=c(2,4,1,1)) # set plotting margins
  par(mfrow=c(2,1))
  # First, plot x vs y, prior and post
  plot(x,y_obs,xlab="x",ylab="y",pch=20,lty=1,col="blue",ylim=c(0,80))
  points(x_post,y_post,pch=20,col="cyan")
  points(x,y_mod,pch=20,col="red")
  points(x_post,y_mod_post,pch=20,col="magenta")
  legend("topleft", legend=c("prior","post","mod prior","mod post"),
       col=c("blue", "cyan","red", "magenta"), lty=1, cex=0.5)
  # Now we'll plot a 1:1 plot
  plot(y_obs,y_mod,ylab="meas",xlab="model",pch=20,col="blue",ylim=c(0,80))
  points(y_post,y_mod_post,pch=20,col="cyan")
  lines(y_obs,y_obs,col="red")
  legend("topleft", legend=c("prior","post"),col=c("blue", "cyan"), lty=1, cex=0.5)
  # We'll also print the root mean squared error
  rmse = (sum((y_obs-y_mod)**2)/length(y_obs))**0.5
  print(paste0("RMSE prior: ",rmse))
  rmse = (sum((y_post-y_mod_post)**2)/length(y_post))**0.5
  print(paste0("RMSE posterior: ",rmse))
}
compare_post_prior(x,y_obs,y_mod,x_post,y_post,y_mod_post)
```

From the MCMC we have gained new estimates of x, the independent variable, which improve agreement with the model but respect the uncertainty ranges in the initial estimate of x. We have new estimates of the observations, y, of the dependent variable, which also respect the prior (eg. observed) values and the uncertainty. We also have an new estimate of the parameter p, and thus also of the modelled values of y.

We can see in the the output that the agreement between y and modelled y has improved from RMSE of 38 to RMSE of 17 following the MCMC. The top figure panel shows the prior and posterior modelled (red/magenta) and data (blue/cyan) relationships between x and y. The bottom panel directly compares input and modelled y. These panels show that the data-model agreement has significantly improved. 

The agreement following the MCMC is not perfect: We may have needed more iterations, and moreover, the model itself has an uncertainty of 2. When using an MCMC in a real study, we run iterations until new iterations make no additional changes in the posterior parameter estimates. We do not expect perfect agreement, as we have initial ideas of our parameters (presumably based on previous investigations) as well as a certain amount of uncertainty in our model.

## Estimate two parameters: Slope and intercept

We will now repeat and edit the MCMC code to estimate not just the slope, which we called p, but additionally the intercept, which we will call q.

```{r}
q = 5
q_err = 2
```

Remember, when we generated the synthetic dataset y_obs, we used an intercept of 2.

We also need to edit the model to use q instead of a set intercept of 2.

```{r}
test_model_2 = function(x,p,q){ 
  return(x*p+q)
}
model_err = 2
```

Let's look at our new dataset and model.

```{r}
y_mod = test_model_2(x,p,q)
compare_obs_model(y_obs,y_obs_err,y_mod)
```

Most parameters of the MCMC remain the same, but we need to reinitialize some.

```{r}
prev_x = x
prev_y = y_obs
prev_p = p
prev_q = q
prev_y_mod = test_model_2(x,p,q)

# Create space for the results of the MCMC
x_i = matrix(nrow=n_iterations,ncol=length(x),-1) # Values of x at each iteration
y_est_i = matrix(nrow=n_iterations,ncol=length(y_obs),-1) # Values of y at each iteration
y_mod_i = matrix(nrow=n_iterations,ncol=length(y_obs),-1) # Modelled values of y at each iteration
p_i = matrix(nrow=n_iterations,ncol=1,-1) # Values of p at each iteration
q_i = matrix(nrow=n_iterations,ncol=1,-1) # Values of p at each iteration
accepted = matrix(nrow=n_iterations,ncol=1,-1) # Signify if iteration accepted or not; 1 = accepted, 0 = not accepted, -1 = initial value (shows us if iteration was run)
```

Now we can run the MCMC. Nearly all the code is the same, we have just added code to handle q.

```{r}
# Run the MCMC
for (n in 1:n_iterations){
  accept = 0
  
  # "Step" the parameters
  x_i[n,] = prev_x + x_err*runif(1,-1,1)*step_length
  y_est_i[n,] = prev_y + y_obs_err*runif(1,-1,1)*step_length
  p_i[n] = prev_p + p_err*runif(1,-1,1)*step_length
  q_i[n] = prev_q + q_err*runif(1,-1,1)*step_length
  
  # Decide if we "accept" the new values of x, y and p, q
  current_prob_x = gaussian_prob(x_i[n,],x,x_err)
  prev_prob_x = gaussian_prob(prev_x,x,x_err)
  if (runif(1,0,1)<current_prob_x/prev_prob_x){accept= accept + 1}
  current_prob_y = gaussian_prob(y_est_i[n,],y_obs,y_obs_err)
  prev_prob_y = gaussian_prob(prev_y,y_obs,y_obs_err)
  if (runif(1,0,1)<current_prob_y/prev_prob_y){accept= accept + 1}
  current_prob_p = gaussian_prob(p_i[n],p,p_err)
  prev_prob_p = gaussian_prob(prev_p,p,p_err)
  if (runif(1,0,1)<current_prob_p/prev_prob_p){accept= accept + 1}
  current_prob_q = gaussian_prob(q_i[n],q,q_err)
  prev_prob_q = gaussian_prob(prev_q,q,q_err)
  if (runif(1,0,1)<current_prob_q/prev_prob_q){accept= accept + 1}
  # If all three new input datasets/values were accepted, accept is == 4
  
  # If all the new values were accepted, run the model
  if (accept==4){
    y_mod_i[n,] = test_model_2(x_i[n,],p_i[n],q_i[n])
    # Test agreement between the modelled results and the estimation of y in this iteration
    current_prob_m = gaussian_prob(y_mod_i[n,],y_est_i[n,],model_err)
    prev_prob_m = gaussian_prob(prev_y_mod,y_est_i[n,],model_err)
    if (runif(1,0,1)<current_prob_m/prev_prob_m){accept= accept + 1}
  }
  
  # Now we have completed the calculations of this iteration of the MCMC
  # If x, p, y and the model results were accepted, we use these values as the starting point of the next iteration
  if (accept==5){
    accepted[n] = 1
    prev_x = x_i[n,]
    prev_y = y_est_i[n,]
    prev_p = p_i[n]
    prev_q = q_i[n]
    prev_y_mod = y_mod_i[n,]
  } else {
    accepted[n] = 0
  }
}

# Check the MCMC ran correctly
print(paste0(n_iterations," iterations were run"))
print(paste0(sum(accepted==-1)," iterations did not run correctly"))
print(paste0(sum(accepted==1)," iterations were accepted"))
print(paste0(sum(accepted==1)/n_iterations*100,"% of iterations were accepted"))
```

Comparing to the first run, we can see we accepted less solutions with one additional constraint.

Let's see what our `posterior' results look like for p and q.

```{r}
par(mfrow=c(1,3))
plot(1:n_iterations,p_i,pch=4,col="blue",ylab="p")
points((1:n_iterations)[accepted==1],p_i[accepted==1],pch=4,col="red")
lines(1:n_iterations,rep(p,n_iterations),col="yellow") # Plot prior p also
legend("topleft", legend=c("prior","all solutions","accepted solutions"),col=c("blue", "red","yellow"), lty=1, cex=0.5, pch=20)
plot(1:n_iterations,q_i,pch=4,col="blue",ylab="q")
points((1:n_iterations)[accepted==1],q_i[accepted==1],pch=4,col="red")
lines(1:n_iterations,rep(q,n_iterations),col="yellow") # Plot prior q also
plot(p_i,q_i,pch=4,col="blue",xlab="p",ylab="q")
points(p_i[accepted==1],q_i[accepted==1],pch=4,col="red")
points(p,q,pch=20,col="yellow")

p_post = c(mean(p_i[accepted==1]),sd(p_i[accepted==1]))
print(paste0("Posterior value of p: p = ",round(p_post[1],2)," +/- ",round(p_post[2],2)))
q_post = c(mean(q_i[accepted==1]),sd(q_i[accepted==1]))
print(paste0("Posterior value of q: q = ",round(q_post[1],2)," +/- ",round(q_post[2],2)))
print(paste0("p-q: Pearson correlation coefficient of  ",round(cor(p_i[accepted==1],q_i[accepted==1]),2)))
```

The estimate of p is similar to the initial MCMC. We see that p and q do not correlate. Often in a real-world MCMC, two parameters will show correlations in accepted results, meaning that their effects may compensate. For example, if we are trying to estimate the global anthropogenic flux of N2O in an MCMC, and we provide ocean and natural terrestrial fluxes as model parameters, their values will compensate as they collectively make up the total natural source.

```{r}
x_post = apply(x_i[accepted==1,],2,mean) # use "apply" to apply the function "mean" to each column (2 = by column, 1 = by row) 
x_post_sd = apply(x_i[accepted==1,],2,sd)
y_post = apply(y_est_i[accepted==1,],2,mean) 
y_post_sd = apply(y_est_i[accepted==1,],2,sd)
y_mod_post = test_model_2(x_post,p_post[1],q_post[1])

compare_post_prior(x,y_obs,y_mod,x_post,y_post,y_mod_post)
```

We were able to optimise both parameters p and q and find a linear model most consistent with our data and our initial estimate of the model. We were not able to produce a perfect agreement between model and data - in a real world setting, this would mean that: i) one or more of our inputs are more uncertain than we estimated, ii) our model lacks processes or represents relationships incorrectly, or a combination of i) and ii). 

In the following exercise, we will investigate the impact of different inputs on the MCMC. In this week's plenary, we will see a real-world example of an MCMC implementation. In Application I (at the end of ESDS I), we will use an MCMC in our analysis of real-world data.




